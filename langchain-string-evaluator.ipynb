{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MqpgsIyOaLdk",
    "outputId": "fa67a911-56e0-4724-a04f-8dfa9fb06249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m897.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m980.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet  langchain langchain-ibm langchain_community pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjLITQN7aGmG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WATSONX_URL=\"\"\n",
    "WATSONX_PROJECT_ID=\"\"\n",
    "WATSONX_API_KEY=\"\"\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = WATSONX_API_KEY\n",
    "os.environ[\"WATSONX_URL\"] = WATSONX_URL\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1F3-hKzktONB"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class WatsonXModel(Enum):\n",
    "    ALL_MINILM_L6_V2 = \"sentence-transformers/all-minilm-l6-v2\"\n",
    "    ALL_MINILM_L12_V2 = \"sentence-transformers/all-minilm-l12-v2\"\n",
    "    ALLAM_1_13B_INSTRUCT = \"sdaia/allam-1-13b-instruct\"\n",
    "    CODELLAMA_34B_INSTRUCT_HF = \"codellama/codellama-34b-instruct-hf\"\n",
    "    ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT = \"elyza/elyza-japanese-llama-2-7b-instruct\"\n",
    "    FLAN_T5_XXL_11B = \"google/flan-t5-xxl\"\n",
    "    FLAN_UL2_20B = \"google/flan-ul2\"\n",
    "    GRANITE_7B_LAB = \"ibm/granite-7b-lab\"\n",
    "    GRANITE_8B_JAPANESE = \"ibm/granite-8b-japanese\"\n",
    "    GRANITE_13B_CHAT_V2 = \"ibm/granite-13b-chat-v2\"\n",
    "    GRANITE_13B_INSTRUCT_V2 = \"ibm/granite-13b-instruct-v2\"\n",
    "    GRANITE_20B_MULTILINGUAL = \"ibm/granite-20b-multilingual\"\n",
    "    GRANITE_3_2B_INSTRUCT = \"ibm/granite-3-2b-instruct\"\n",
    "    GRANITE_3_8B_INSTRUCT = \"ibm/granite-3-8b-instruct\"\n",
    "    GRANITE_GUARDIAN_3_2B = \"ibm/granite-guardian-3-2b\"\n",
    "    GRANITE_GUARDIAN_3_8B = \"ibm/granite-guardian-3-8b\"\n",
    "    GRANITE_3B_CODE_INSTRUCT = \"ibm/granite-3b-code-instruct\"\n",
    "    GRANITE_8B_CODE_INSTRUCT = \"ibm/granite-8b-code-instruct\"\n",
    "    GRANITE_20B_CODE_INSTRUCT = \"ibm/granite-20b-code-instruct\"\n",
    "    GRANITE_34B_CODE_INSTRUCT = \"ibm/granite-34b-code-instruct\"\n",
    "    JAIS_13B_CHAT = \"core42/jais-13b-chat\"\n",
    "    LLAMA_3_2_1B_INSTRUCT = \"meta-llama/llama-3-2-1b-instruct\"\n",
    "    LLAMA_3_2_3B_INSTRUCT = \"meta-llama/llama-3-2-3b-instruct\"\n",
    "    LLAMA_3_2_11B_VISION_INSTRUCT = \"meta-llama/llama-3-2-11b-vision-instruct\"\n",
    "    LLAMA_3_2_90B_VISION_INSTRUCT = \"meta-llama/llama-3-2-90b-vision-instruct\"\n",
    "    LLAMA_GUARD_3_11B_INSTRUCT = \"meta-llama/llama-guard-3-11b-vision\"\n",
    "    LLAMA3_LLAVA_NEXT_8B_HF = \"meta-llama/llama3-llava-next-8b-hf\"\n",
    "    LLAMA_3_1_8B_INSTRUCT = \"meta-llama/llama-3-1-8b-instruct\"\n",
    "    LLAMA_3_1_70B_INSTRUCT = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    LLAMA_3_405B_INSTRUCT = \"meta-llama/llama-3-405b-instruct\"\n",
    "    LLAMA_3_8B_INSTRUCT = \"meta-llama/llama-3-8b-instruct\"\n",
    "    LLAMA_3_70B_INSTRUCT = \"meta-llama/llama-3-70b-instruct\"\n",
    "    LLAMA_2_13B_CHAT = \"meta-llama/llama-2-13b-chat\"\n",
    "    LLAMA2_13B_DPO_V7 = \"mnci/llama2-13b-dpo-v7\"\n",
    "    MISTRAL_LARGE = \"mistralai/mistral-large\"\n",
    "    MIXTRAL_8X7B_INSTRUCT_V01 = \"mistralai/mixtral-8x7b-instruct-v01\"\n",
    "    MS_MARCO_MINILM_L_12_V2 = \"cross-encoder/ms-marco-minilm-l-12-v2\"\n",
    "    MT0_XXL_13B = \"bigscience/mt0-xxl\"\n",
    "    MULTILINGUAL_E5_LARGE = \"intfloat/multilingual-e5-large\"\n",
    "    PIXTRAL_12B = \"mistralai/pixtral-12b\"\n",
    "    SLATE_30M_ENGLISH_RTRVR = \"ibm/slate-30m-english-rtrvr\"\n",
    "    SLATE_30M_ENGLISH_RTRVR_V2 = \"ibm/slate-30m-english-rtrvr-v2\"\n",
    "    SLATE_125M_ENGLISH_RTRVR = \"ibm/slate-125m-english-rtrvr\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Metric LangChain String Evaluation (Non-Configurable Judge LLM Model Parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "gqIOOi9n1Jxp",
    "outputId": "e520badf-0d1c-4507-958b-f47a01665274"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from IPython.display import display\n",
    "from typing import List, Dict\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Configuration Parameters for LLM\n",
    "STOP_SEQUENCE=\"###\"\n",
    "\n",
    "parameters = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [STOP_SEQUENCE],\n",
    "}\n",
    "\n",
    "# Evaluation Criteria\n",
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 2: The answer has minor relevance but does not align with the reference.\n",
    "Score 3: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 4: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 5: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}\n",
    "\n",
    "# Dataset File Path\n",
    "DATASET_FILE_PATH = 'test-data.xlsx'  # Replace with your file path\n",
    "\n",
    "# Constants for Retry Mechanism\n",
    "MAX_RETRIES = 3  # Maximum number of retry attempts\n",
    "\n",
    "# Define the PromptTemplate with system and user prompts\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert assistant that provides helpful and accurate answers.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\",\n",
    "    input_variables=[\"instruction\"],\n",
    ")\n",
    "\n",
    "selected_models = [WatsonXModel.MISTRAL_LARGE, WatsonXModel.LLAMA_3_8B_INSTRUCT, WatsonXModel.GRANITE_3_8B_INSTRUCT]  # Example selection\n",
    "\n",
    "\n",
    "def get_evaluation_criteria(custom_criteria: Dict[str, str] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Combine default accuracy criteria with any custom criteria provided.\n",
    "    \"\"\"\n",
    "    default_criteria = accuracy_criteria.copy()\n",
    "    if custom_criteria:\n",
    "        default_criteria.update(custom_criteria)\n",
    "    print(\"Evaluation criteria loaded.\")\n",
    "    return default_criteria\n",
    "\n",
    "def load_queries_from_excel(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load queries and expected answers from an Excel file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading queries from Excel file: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "        raise\n",
    "\n",
    "    required_columns = ['Query', 'Expected']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    print(f\"Loaded {len(df)} queries from Excel.\")\n",
    "    return df\n",
    "\n",
    "def initialize_llm(model_id: str):\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM with the given model ID.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing LLM with model ID: {model_id}\")\n",
    "    try:\n",
    "        llm = WatsonxLLM(\n",
    "            model_id=model_id,\n",
    "            url=WATSONX_URL,\n",
    "            project_id=WATSONX_PROJECT_ID,\n",
    "            params=parameters,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize LLM {model_id}: {e}\")\n",
    "        raise\n",
    "    print(f\"LLM {model_id} initialized.\")\n",
    "    return llm\n",
    "\n",
    "def generate_predictions(llm, queries: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate predictions for a list of queries using the provided LLM.\n",
    "    Utilizes the prompt template and includes a stop sequence.\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = []\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"Processing query {i}/{len(queries)}: {query}\")\n",
    "        try:\n",
    "            # Format the prompt using the prompt template\n",
    "            formatted_prompt = prompt_template.format(instruction=query)\n",
    "            print(f\"Formatted Prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "            # Invoke the LLM with the formatted prompt and stop sequence\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            # Optionally, remove the stop sequence from the response\n",
    "            for stop_seq in STOP_SEQUENCE:\n",
    "                response = response.split(stop_seq)[0].strip()\n",
    "\n",
    "            print(f\"Prediction for query {i}: {response}\")\n",
    "            predictions.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction for query {i}: {e}\")\n",
    "            predictions.append(\"Error generating prediction\")\n",
    "    print(\"Finished generating predictions.\")\n",
    "    return predictions\n",
    "\n",
    "def evaluate_predictions(evaluator, predictions: List[str], references: List[str], inputs: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against references using the provided evaluator.\n",
    "    Implements a retry mechanism for robustness.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating predictions...\")\n",
    "    eval_results = []\n",
    "    for i, (pred, ref, inp) in enumerate(zip(predictions, references, inputs), 1):\n",
    "        print(f\"Evaluating prediction {i}/{len(predictions)}\")\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        while attempt < MAX_RETRIES and not success:\n",
    "            try:\n",
    "                result = evaluator.evaluate_strings(\n",
    "                    prediction=pred,\n",
    "                    reference=ref,\n",
    "                    input=inp\n",
    "                )\n",
    "                eval_results.append(result)\n",
    "                print(f\"Evaluation result for prediction {i}: {result}\")\n",
    "                success = True\n",
    "            except ValueError as ve:\n",
    "                attempt += 1\n",
    "                print(f\"ValueError during evaluation of prediction {i}: {ve}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    print(f\"Retrying evaluation for prediction {i} (Attempt {attempt + 1}/{MAX_RETRIES})...\")\n",
    "                else:\n",
    "                    print(f\"Max retries reached for prediction {i}. Assigning default evaluation results.\")\n",
    "                    eval_results.append({\n",
    "                        'score': 'N/A',\n",
    "                        'reasoning': 'Evaluation failed after multiple attempts.'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Unexpected error during evaluation of prediction {i}: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    print(f\"Retrying evaluation for prediction {i} (Attempt {attempt + 1}/{MAX_RETRIES})...\")\n",
    "                else:\n",
    "                    print(f\"Max retries reached for prediction {i}. Assigning default evaluation results.\")\n",
    "                    eval_results.append({\n",
    "                        'score': 'N/A',\n",
    "                        'reasoning': 'Evaluation failed after multiple attempts due to an unexpected error.'\n",
    "                    })\n",
    "    print(\"Finished evaluating predictions.\")\n",
    "    return eval_results\n",
    "\n",
    "def main():\n",
    "    print(\"Starting evaluation process...\")\n",
    "\n",
    "    # Configuration Section\n",
    "    custom_criteria = {}  # Add custom criteria here if needed\n",
    "    criteria = get_evaluation_criteria(custom_criteria)\n",
    "\n",
    "    print(f\"Selected models: {[model.name for model in selected_models]}\")\n",
    "\n",
    "    # Load Queries from Excel\n",
    "    df = load_queries_from_excel(DATASET_FILE_PATH)\n",
    "    queries = df['Query'].tolist()\n",
    "    print(f\"Loaded queries: {queries}\")\n",
    "    df['ExpectedList'] = df['Expected'].apply(lambda x: [e.strip() for e in str(x).split(';')])\n",
    "    references = df['ExpectedList'].apply(lambda x: x[0] if len(x) > 0 else '').tolist()\n",
    "    print(f\"Loaded references: {references}\")\n",
    "\n",
    "    # Initialize Evaluator\n",
    "    try:\n",
    "        evaluator_llm = initialize_llm(WatsonXModel.MISTRAL_LARGE.value)  # Use .value to get the model ID string\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize evaluator LLM: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        evaluator = load_evaluator(\n",
    "            \"labeled_score_string\",\n",
    "            criteria=criteria,\n",
    "            llm=evaluator_llm  # Placeholder; actual LLM will be used per model\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load evaluator: {e}\")\n",
    "        return\n",
    "    print(\"Evaluator initialized.\")\n",
    "\n",
    "    # Prepare a DataFrame to store results\n",
    "    results = pd.DataFrame(\n",
    "        columns=[\"Model\", \"Query\", \"Reference\", \"Prediction\", \"Score\", \"Feedback\"]\n",
    "    )\n",
    "\n",
    "    # Iterate over each selected model\n",
    "    for model in selected_models:\n",
    "        print(f\"Evaluating with model: {model.name}\")\n",
    "        try:\n",
    "            llm = initialize_llm(model.value)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {model.name} due to initialization failure: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate predictions using the prompt template\n",
    "        predictions = generate_predictions(llm, queries)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        eval_results = evaluate_predictions(evaluator, predictions, references, queries)\n",
    "\n",
    "        # Create a DataFrame with the new results\n",
    "        new_results = pd.DataFrame({\n",
    "            'Model': model.name,\n",
    "            'Query': queries,\n",
    "            'Reference': references,\n",
    "            'Prediction': predictions,\n",
    "            # Extract 'score' from eval results or set 'N/A' if missing\n",
    "            'Score': [eval_res.get('score', 'N/A') for eval_res in eval_results],\n",
    "            # Extract 'feedback' from eval results, or use an empty string if missing\n",
    "            'Feedback': [eval_res.get('reasoning', '') for eval_res in eval_results]\n",
    "        })\n",
    "\n",
    "        # Concatenate the new results to the existing DataFrame\n",
    "        results = pd.concat([results, new_results], ignore_index=True)\n",
    "        print(f\"Results updated for model: {model.name}\")\n",
    "\n",
    "    # Display Results\n",
    "    print(\"Displaying results...\")\n",
    "    display(results)\n",
    "\n",
    "    # Optionally, save results to Excel\n",
    "    try:\n",
    "        results.to_excel('evaluation_results.xlsx', index=False)\n",
    "        print(\"Evaluation results saved to 'evaluation_results.xlsx'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save evaluation results to Excel: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Metric LangChain String Evaluation (Configurable Judge LLM Model Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvkATiw4AbSB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum, auto\n",
    "from langchain import PromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from IPython.display import display\n",
    "from typing import List, Dict\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Configuration Parameters for LLM Generation\n",
    "GENERATION_STOP_SEQUENCE = \"###\"\n",
    "\n",
    "generation_parameters = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [GENERATION_STOP_SEQUENCE],\n",
    "}\n",
    "\n",
    "# Configuration Parameters for Judge Model\n",
    "JUDGE_STOP_SEQUENCE = \"###\"\n",
    "\n",
    "judge_parameters = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 150,  # Increased tokens for detailed feedback\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stop_sequences\": [JUDGE_STOP_SEQUENCE],\n",
    "}\n",
    "\n",
    "# Evaluation Criteria for Multiple Metrics\n",
    "evaluation_metrics = {\n",
    "    \"Accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 2: The answer has minor relevance but does not align with the reference.\n",
    "Score 3: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 4: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 5: The answer is completely accurate and aligns perfectly with the reference.\"\"\",\n",
    "    \"Fluency\": \"\"\"\n",
    "Score 1: The translation has numerous spelling, punctuation, and grammatical errors.\n",
    "Score 2: The translation has several errors that hinder readability.\n",
    "Score 3: The translation has some errors but is generally understandable.\n",
    "Score 4: The translation has minor errors that do not significantly affect readability.\n",
    "Score 5: The translation is free of spelling, punctuation, and grammatical errors.\"\"\",\n",
    "    \"Terminology\": \"\"\"\n",
    "Score 1: Keywords and phrases are inaccurately translated or inconsistent.\n",
    "Score 2: Several keywords and phrases are inaccurately translated.\n",
    "Score 3: Some keywords and phrases are accurately translated, but there are inconsistencies.\n",
    "Score 4: Most keywords and phrases are accurately translated with minor inconsistencies.\n",
    "Score 5: All keywords and phrases are accurately and consistently translated.\"\"\",\n",
    "    \"Style\": \"\"\"\n",
    "Score 1: The translation is difficult to read and does not match the style of the original.\n",
    "Score 2: The translation has significant style issues that affect readability.\n",
    "Score 3: The translation is readable but only partially matches the original style.\n",
    "Score 4: The translation is easy to read and mostly matches the original style.\n",
    "Score 5: The translation is easy to read, understand, and perfectly matches the style of the original.\"\"\",\n",
    "    \"Design\": \"\"\"\n",
    "Score 1: The translation has major formatting issues that disrupt the structure.\n",
    "Score 2: The translation has several formatting errors.\n",
    "Score 3: The translation has some formatting inconsistencies.\n",
    "Score 4: The translation is mostly well-formatted with minor issues.\n",
    "Score 5: The translation is perfectly formatted with no issues.\"\"\",\n",
    "}\n",
    "\n",
    "# Dataset File Path\n",
    "DATASET_FILE_PATH = \"test-data.xlsx\"  # Replace with your file path\n",
    "\n",
    "# Constants for Retry Mechanism\n",
    "MAX_RETRIES = 3  # Maximum number of retry attempts\n",
    "\n",
    "# Define the PromptTemplate with system and user prompts\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert assistant that provides helpful and accurate answers.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\",\n",
    "    input_variables=[\"instruction\"],\n",
    ")\n",
    "\n",
    "# Example selection of models\n",
    "selected_models = [\n",
    "    WatsonXModel.MISTRAL_LARGE,\n",
    "    # WatsonXModel.LLAMA_3_8B_INSTRUCT,\n",
    "    # WatsonXModel.GRANITE_3_8B_INSTRUCT\n",
    "]\n",
    "\n",
    "\n",
    "def get_evaluators(\n",
    "    metrics: Dict[str, str], judge_model_id: str, judge_params: Dict\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Initialize evaluators for each metric.\n",
    "    \"\"\"\n",
    "    evaluators = {}\n",
    "    try:\n",
    "        judge_llm = WatsonxLLM(\n",
    "            model_id=judge_model_id,\n",
    "            url=WATSONX_URL,\n",
    "            project_id=WATSONX_PROJECT_ID,\n",
    "            params=judge_params,\n",
    "        )\n",
    "        print(f\"Judge LLM '{judge_model_id}' initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Judge LLM '{judge_model_id}': {e}\")\n",
    "        raise\n",
    "\n",
    "    for metric_name, criteria in metrics.items():\n",
    "        try:\n",
    "            evaluator = load_evaluator(\n",
    "                \"labeled_score_string\", criteria=criteria, llm=judge_llm\n",
    "            )\n",
    "            evaluators[metric_name] = evaluator\n",
    "            print(f\"Evaluator for metric '{metric_name}' loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load evaluator for metric '{metric_name}': {e}\")\n",
    "            raise\n",
    "    return evaluators\n",
    "\n",
    "\n",
    "def load_queries_from_excel(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load queries and expected answers from an Excel file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading queries from Excel file: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "        raise\n",
    "\n",
    "    required_columns = [\"Query\", \"Expected\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    print(f\"Loaded {len(df)} queries from Excel.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def initialize_llm(model_id: str, params: Dict) -> WatsonxLLM:\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM with the given model ID and parameters.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing LLM with model ID: {model_id}\")\n",
    "    try:\n",
    "        llm = WatsonxLLM(\n",
    "            model_id=model_id,\n",
    "            url=WATSONX_URL,\n",
    "            project_id=WATSONX_PROJECT_ID,\n",
    "            params=params,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize LLM '{model_id}': {e}\")\n",
    "        raise\n",
    "    print(f\"LLM '{model_id}' initialized.\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def generate_predictions(llm: WatsonxLLM, queries: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate predictions for a list of queries using the provided LLM.\n",
    "    Utilizes the prompt template and includes a stop sequence.\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = []\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"Processing query {i}/{len(queries)}: {query}\")\n",
    "        try:\n",
    "            # Format the prompt using the prompt template\n",
    "            formatted_prompt = prompt_template.format(instruction=query)\n",
    "            print(f\"Formatted Prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "            # Invoke the LLM with the formatted prompt and stop sequence\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            # Optionally, remove the stop sequence from the response\n",
    "            for stop_seq in llm.params.get(\"stop_sequences\", []):\n",
    "                response = response.split(stop_seq)[0].strip()\n",
    "\n",
    "            print(f\"Prediction for query {i}: {response}\")\n",
    "            predictions.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction for query {i}: {e}\")\n",
    "            predictions.append(\"Error generating prediction\")\n",
    "    print(\"Finished generating predictions.\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    evaluators: Dict[str, any],\n",
    "    predictions: List[str],\n",
    "    references: List[str],\n",
    "    inputs: List[str],\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against references using the provided evaluators.\n",
    "    Implements a retry mechanism for robustness.\n",
    "    Returns a list of dictionaries containing metric evaluations.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating predictions...\")\n",
    "    eval_results = []\n",
    "    for i, (pred, ref, inp) in enumerate(zip(predictions, references, inputs), 1):\n",
    "        print(f\"Evaluating prediction {i}/{len(predictions)}\")\n",
    "        for metric_name, evaluator in evaluators.items():\n",
    "            attempt = 0\n",
    "            success = False\n",
    "            while attempt < MAX_RETRIES and not success:\n",
    "                try:\n",
    "                    result = evaluator.evaluate_strings(\n",
    "                        prediction=pred, reference=ref, input=inp\n",
    "                    )\n",
    "                    eval_results.append(\n",
    "                        {\n",
    "                            \"Metric\": metric_name,\n",
    "                            \"Score\": result.get(\"score\", \"N/A\"),\n",
    "                            \"Feedback\": result.get(\"reasoning\", \"\"),\n",
    "                        }\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Evaluation for metric '{metric_name}' on prediction {i}: {result}\"\n",
    "                    )\n",
    "                    success = True\n",
    "                except ValueError as ve:\n",
    "                    attempt += 1\n",
    "                    print(\n",
    "                        f\"ValueError during evaluation of prediction {i} for metric '{metric_name}': {ve}\"\n",
    "                    )\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        print(\n",
    "                            f\"Retrying evaluation for prediction {i}, metric '{metric_name}' (Attempt {attempt + 1}/{MAX_RETRIES})...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Max retries reached for prediction {i}, metric '{metric_name}'. Assigning default evaluation results.\"\n",
    "                        )\n",
    "                        eval_results.append(\n",
    "                            {\n",
    "                                \"Metric\": metric_name,\n",
    "                                \"Score\": \"N/A\",\n",
    "                                \"Feedback\": \"Evaluation failed after multiple attempts.\",\n",
    "                            }\n",
    "                        )\n",
    "                        success = True  # To exit the retry loop\n",
    "                except Exception as e:\n",
    "                    attempt += 1\n",
    "                    print(\n",
    "                        f\"Unexpected error during evaluation of prediction {i} for metric '{metric_name}': {e}\"\n",
    "                    )\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        print(\n",
    "                            f\"Retrying evaluation for prediction {i}, metric '{metric_name}' (Attempt {attempt + 1}/{MAX_RETRIES})...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Max retries reached for prediction {i}, metric '{metric_name}'. Assigning default evaluation results.\"\n",
    "                        )\n",
    "                        eval_results.append(\n",
    "                            {\n",
    "                                \"Metric\": metric_name,\n",
    "                                \"Score\": \"N/A\",\n",
    "                                \"Feedback\": \"Evaluation failed after multiple attempts due to an unexpected error.\",\n",
    "                            }\n",
    "                        )\n",
    "                        success = True  # To exit the retry loop\n",
    "    print(\"Finished evaluating predictions.\")\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting evaluation process...\")\n",
    "\n",
    "    # Configuration Section\n",
    "    criteria = evaluation_metrics  # Use predefined evaluation metrics\n",
    "\n",
    "    # Initialize Evaluators\n",
    "    try:\n",
    "        evaluators = get_evaluators(\n",
    "            metrics=criteria,\n",
    "            judge_model_id=WatsonXModel.MISTRAL_LARGE.value,  # Replace with appropriate judge model\n",
    "            judge_params=judge_parameters,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize evaluators: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Selected models: {[model.name for model in selected_models]}\")\n",
    "\n",
    "    # Load Queries from Excel\n",
    "    df = load_queries_from_excel(DATASET_FILE_PATH)\n",
    "    queries = df[\"Query\"].tolist()\n",
    "    print(f\"Loaded queries: {queries}\")\n",
    "    df[\"ExpectedList\"] = df[\"Expected\"].apply(\n",
    "        lambda x: [e.strip() for e in str(x).split(\";\")]\n",
    "    )\n",
    "    references = df[\"ExpectedList\"].apply(lambda x: x[0] if len(x) > 0 else \"\").tolist()\n",
    "    print(f\"Loaded references: {references}\")\n",
    "\n",
    "    # Prepare a list to store all result rows\n",
    "    results_list = []\n",
    "\n",
    "    # Iterate over each selected model\n",
    "    for model in selected_models:\n",
    "        print(f\"\\nEvaluating with model: {model.name}\")\n",
    "        try:\n",
    "            llm = initialize_llm(model.value, generation_parameters)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {model.name} due to initialization failure: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate predictions using the prompt template\n",
    "        predictions = generate_predictions(llm, queries)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        eval_results = evaluate_predictions(\n",
    "            evaluators, predictions, references, queries\n",
    "        )\n",
    "\n",
    "        # Organize evaluation results per prediction\n",
    "        num_metrics = len(evaluation_metrics)\n",
    "        for i, query in enumerate(queries):\n",
    "            prediction = predictions[i]\n",
    "            reference = references[i]\n",
    "            for metric_idx in range(num_metrics):\n",
    "                eval_idx = i * num_metrics + metric_idx\n",
    "                eval_res = eval_results[eval_idx]\n",
    "                # Append the result as a dictionary to the results_list\n",
    "                results_list.append(\n",
    "                    {\n",
    "                        \"Model\": model.name,\n",
    "                        \"Query\": query,\n",
    "                        \"Reference\": reference,\n",
    "                        \"Prediction\": prediction,\n",
    "                        \"Metric\": eval_res.get(\"Metric\", \"N/A\"),\n",
    "                        \"Score\": eval_res.get(\"Score\", \"N/A\"),\n",
    "                        \"Feedback\": eval_res.get(\"Feedback\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "        print(f\"Results updated for model: {model.name}\")\n",
    "\n",
    "    # Convert the list of results to a DataFrame\n",
    "    results = pd.DataFrame(\n",
    "        results_list,\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"Query\",\n",
    "            \"Reference\",\n",
    "            \"Prediction\",\n",
    "            \"Metric\",\n",
    "            \"Score\",\n",
    "            \"Feedback\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Display Results\n",
    "    print(\"\\nDisplaying results...\")\n",
    "    display(results)\n",
    "\n",
    "    # Optionally, save results to Excel\n",
    "    try:\n",
    "        results.to_excel(\"evaluation_results.xlsx\", index=False)\n",
    "        print(\"Evaluation results saved to 'evaluation_results.xlsx'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save evaluation results to Excel: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
