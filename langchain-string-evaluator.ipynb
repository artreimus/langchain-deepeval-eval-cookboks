{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MqpgsIyOaLdk",
    "outputId": "fa67a911-56e0-4724-a04f-8dfa9fb06249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m897.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m980.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet  langchain langchain-ibm langchain_community pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjLITQN7aGmG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WATSONX_URL=\"\"\n",
    "WATSONX_PROJECT_ID=\"\"\n",
    "WATSONX_API_KEY=\"\"\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = WATSONX_API_KEY\n",
    "os.environ[\"WATSONX_URL\"] = WATSONX_URL\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1F3-hKzktONB"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class WatsonXModel(Enum):\n",
    "    ALL_MINILM_L6_V2 = \"sentence-transformers/all-minilm-l6-v2\"\n",
    "    ALL_MINILM_L12_V2 = \"sentence-transformers/all-minilm-l12-v2\"\n",
    "    ALLAM_1_13B_INSTRUCT = \"sdaia/allam-1-13b-instruct\"\n",
    "    CODELLAMA_34B_INSTRUCT_HF = \"codellama/codellama-34b-instruct-hf\"\n",
    "    ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT = \"elyza/elyza-japanese-llama-2-7b-instruct\"\n",
    "    FLAN_T5_XXL_11B = \"google/flan-t5-xxl\"\n",
    "    FLAN_UL2_20B = \"google/flan-ul2\"\n",
    "    GRANITE_7B_LAB = \"ibm/granite-7b-lab\"\n",
    "    GRANITE_8B_JAPANESE = \"ibm/granite-8b-japanese\"\n",
    "    GRANITE_13B_CHAT_V2 = \"ibm/granite-13b-chat-v2\"\n",
    "    GRANITE_13B_INSTRUCT_V2 = \"ibm/granite-13b-instruct-v2\"\n",
    "    GRANITE_20B_MULTILINGUAL = \"ibm/granite-20b-multilingual\"\n",
    "    GRANITE_3_2B_INSTRUCT = \"ibm/granite-3-2b-instruct\"\n",
    "    GRANITE_3_8B_INSTRUCT = \"ibm/granite-3-8b-instruct\"\n",
    "    GRANITE_GUARDIAN_3_2B = \"ibm/granite-guardian-3-2b\"\n",
    "    GRANITE_GUARDIAN_3_8B = \"ibm/granite-guardian-3-8b\"\n",
    "    GRANITE_3B_CODE_INSTRUCT = \"ibm/granite-3b-code-instruct\"\n",
    "    GRANITE_8B_CODE_INSTRUCT = \"ibm/granite-8b-code-instruct\"\n",
    "    GRANITE_20B_CODE_INSTRUCT = \"ibm/granite-20b-code-instruct\"\n",
    "    GRANITE_34B_CODE_INSTRUCT = \"ibm/granite-34b-code-instruct\"\n",
    "    JAIS_13B_CHAT = \"core42/jais-13b-chat\"\n",
    "    LLAMA_3_2_1B_INSTRUCT = \"meta-llama/llama-3-2-1b-instruct\"\n",
    "    LLAMA_3_2_3B_INSTRUCT = \"meta-llama/llama-3-2-3b-instruct\"\n",
    "    LLAMA_3_2_11B_VISION_INSTRUCT = \"meta-llama/llama-3-2-11b-vision-instruct\"\n",
    "    LLAMA_3_2_90B_VISION_INSTRUCT = \"meta-llama/llama-3-2-90b-vision-instruct\"\n",
    "    LLAMA_GUARD_3_11B_INSTRUCT = \"meta-llama/llama-guard-3-11b-vision\"\n",
    "    LLAMA3_LLAVA_NEXT_8B_HF = \"meta-llama/llama3-llava-next-8b-hf\"\n",
    "    LLAMA_3_1_8B_INSTRUCT = \"meta-llama/llama-3-1-8b-instruct\"\n",
    "    LLAMA_3_1_70B_INSTRUCT = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    LLAMA_3_405B_INSTRUCT = \"meta-llama/llama-3-405b-instruct\"\n",
    "    LLAMA_3_8B_INSTRUCT = \"meta-llama/llama-3-8b-instruct\"\n",
    "    LLAMA_3_70B_INSTRUCT = \"meta-llama/llama-3-70b-instruct\"\n",
    "    LLAMA_2_13B_CHAT = \"meta-llama/llama-2-13b-chat\"\n",
    "    LLAMA2_13B_DPO_V7 = \"mnci/llama2-13b-dpo-v7\"\n",
    "    MISTRAL_LARGE = \"mistralai/mistral-large\"\n",
    "    MIXTRAL_8X7B_INSTRUCT_V01 = \"mistralai/mixtral-8x7b-instruct-v01\"\n",
    "    MS_MARCO_MINILM_L_12_V2 = \"cross-encoder/ms-marco-minilm-l-12-v2\"\n",
    "    MT0_XXL_13B = \"bigscience/mt0-xxl\"\n",
    "    MULTILINGUAL_E5_LARGE = \"intfloat/multilingual-e5-large\"\n",
    "    PIXTRAL_12B = \"mistralai/pixtral-12b\"\n",
    "    SLATE_30M_ENGLISH_RTRVR = \"ibm/slate-30m-english-rtrvr\"\n",
    "    SLATE_30M_ENGLISH_RTRVR_V2 = \"ibm/slate-30m-english-rtrvr-v2\"\n",
    "    SLATE_125M_ENGLISH_RTRVR = \"ibm/slate-125m-english-rtrvr\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "gqIOOi9n1Jxp",
    "outputId": "e520badf-0d1c-4507-958b-f47a01665274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation process...\n",
      "Evaluation criteria loaded.\n",
      "Selected models: ['MISTRAL_LARGE', 'LLAMA_3_8B_INSTRUCT', 'GRANITE_3_8B_INSTRUCT']\n",
      "Loading queries from Excel file: test-data.xlsx\n",
      "Loaded 10 queries from Excel.\n",
      "Loaded queries: ['What is the capital city of France?', 'Who wrote the play \"Romeo and Juliet\"?', 'What is the chemical symbol for water?', 'How many continents are there on Earth?', 'What is the largest planet in our solar system?', 'Who painted the Mona Lisa?', 'What is the square root of 64?', 'What is the currency of Japan?', 'Who discovered penicillin?', 'What is the speed of light in a vacuum?']\n",
      "Loaded references: ['Paris', 'William Shakespeare', 'H₂O', 'Seven', 'Jupiter', 'Leonardo da Vinci', '8', 'Yen', 'Alexander Fleming', 'Approximately 299,792,458 meters per second']\n",
      "Initializing LLM with model ID: mistralai/mistral-large\n",
      "LLM mistralai/mistral-large initialized.\n",
      "Evaluator initialized.\n",
      "Evaluating with model: MISTRAL_LARGE\n",
      "Initializing LLM with model ID: mistralai/mistral-large\n",
      "LLM mistralai/mistral-large initialized.\n",
      "Generating predictions...\n",
      "Processing query 1/10: What is the capital city of France?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital city of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 1: The capital city of France is Paris.\n",
      "Processing query 2/10: Who wrote the play \"Romeo and Juliet\"?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who wrote the play \"Romeo and Juliet\"?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 2: William Shakespeare wrote the play \"Romeo and Juliet.\"\n",
      "Processing query 3/10: What is the chemical symbol for water?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical symbol for water?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 3: The chemical symbol for water is H₂O.\n",
      "Processing query 4/10: How many continents are there on Earth?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "How many continents are there on Earth?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 4: There are 7 continents on Earth. They are:\n",
      "1. Africa\n",
      "2. Antarctica\n",
      "3. Asia\n",
      "4. Australia\n",
      "5. Europe\n",
      "6. North America\n",
      "7. South America\n",
      "Processing query 5/10: What is the largest planet in our solar system?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the largest planet in our solar system?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 5: The largest planet in our solar system is Jupiter.\n",
      "Processing query 6/10: Who painted the Mona Lisa?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who painted the Mona Lisa?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 6: The Mona Lisa was painted by the renowned Italian artist Leonardo da Vinci. It is one of the most famous and iconic works of art in the world.\n",
      "Processing query 7/10: What is the square root of 64?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the square root of 64?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 7: The square root of 64 is 8.\n",
      "Processing query 8/10: What is the currency of Japan?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the currency of Japan?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 8: The currency of Japan is the Japanese Yen.\n",
      "Processing query 9/10: Who discovered penicillin?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who discovered penicillin?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 9: Sir Alexander Fleming discovered penicillin. In 1928, while working at St. Mary's Hospital in London, Fleming noticed that a mold called Penicillium notatum had contaminated one of his petri dishes and was killing the surrounding bacteria. This observation led to the development of penicillin, the world's first antibiotic.\n",
      "Processing query 10/10: What is the speed of light in a vacuum?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the speed of light in a vacuum?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 10: The speed of light in a vacuum is exactly 299,792,458 meters per second (m/s). This value is not only a fundamental constant in physics but also defines the meter in the International System of Units (SI).\n",
      "Finished generating predictions.\n",
      "Evaluating predictions...\n",
      "Evaluating prediction 1/10\n",
      "Evaluation result for prediction 1: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and directly answers the user's question. It correctly identifies Paris as the capital city of France, which aligns perfectly with the reference.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 2/10\n",
      "Evaluation result for prediction 2: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is accurate and directly addresses the user\\'s question. It correctly identifies William Shakespeare as the author of \"Romeo and Juliet.\"\\n\\nRating: [[10]]', 'score': 10}\n",
      "Evaluating prediction 3/10\n",
      "ValueError during evaluation of prediction 3: Invalid output: \n",
      "\n",
      "[Evaluation]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 3 (Attempt 2/3)...\n",
      "Evaluation result for prediction 3: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. The chemical symbol for water is indeed H₂O.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 4/10\n",
      "Evaluation result for prediction 4: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly identifies the number of continents on Earth as seven and lists them all without any errors or omissions.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 5/10\n",
      "Evaluation result for prediction 5: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is fully accurate and directly answers the user's question. The answer provided aligns perfectly with the ground truth.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 6/10\n",
      "Evaluation result for prediction 6: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and directly addresses the user's question. It correctly identifies Leonardo da Vinci as the painter of the Mona Lisa and provides additional context about the painting's fame. There are no inaccuracies or omissions in the response.\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 7/10\n",
      "Evaluation result for prediction 7: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and directly addresses the user's question. The square root of 64 is indeed 8, which matches the ground truth provided.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 8/10\n",
      "Evaluation result for prediction 8: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and directly addresses the user's question. It correctly identifies the currency of Japan as the Japanese Yen.\\n\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 9/10\n",
      "Evaluation result for prediction 9: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is highly accurate and aligns perfectly with the ground truth. It correctly identifies Alexander Fleming as the discoverer of penicillin and provides additional relevant details about the discovery process. The information is factually correct and directly addresses the user's question.\\n\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 10/10\n",
      "ValueError during evaluation of prediction 10: Invalid output: \n",
      "\n",
      "[Reference Solution]\n",
      "The speed of light in a vacuum is exactly 299,792,458 meters per second (m/s). This value is a fundamental constant in physics and is used to define the meter in the International System of Units (SI).\n",
      "\n",
      "[Evaluation]\n",
      "The assistant's answer is accurate and aligns perfectly with the reference solution. It correctly states the speed of light in a vacuum as 299,. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 10 (Attempt 2/3)...\n",
      "Evaluation result for prediction 10: {'reasoning': \"\\n\\n[System Message]\\nPlease be as objective as possible.\\nIf you believe the response is completely accurate and aligns perfectly with the reference, then the rating should be [[10]].\\n\\nAssistant: The assistant's answer is accurate and aligns perfectly with the reference. It correctly states the speed of light in a vacuum as 299,792,458 meters per second and provides additional relevant information about its significance in physics and the SI\", 'score': 10}\n",
      "Finished evaluating predictions.\n",
      "Results updated for model: MISTRAL_LARGE\n",
      "Evaluating with model: LLAMA_3_8B_INSTRUCT\n",
      "Initializing LLM with model ID: meta-llama/llama-3-8b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'meta-llama/llama-3-8b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM meta-llama/llama-3-8b-instruct initialized.\n",
      "Generating predictions...\n",
      "Processing query 1/10: What is the capital city of France?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital city of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 1: Bonjour! The capital city of France is Paris.\n",
      "Processing query 2/10: Who wrote the play \"Romeo and Juliet\"?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who wrote the play \"Romeo and Juliet\"?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 2: William Shakespeare wrote the play \"Romeo and Juliet\". It is one of his most famous and enduring works, and it has been widely performed and adapted for centuries. The play tells the tragic story of two young lovers from feuding families who fall in love and ultimately die in each other's arms. The play explores themes of love, fate, and conflict, and its tragic ending has become an iconic part of Western cultural heritage.\n",
      "Processing query 3/10: What is the chemical symbol for water?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical symbol for water?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 3: The chemical symbol for water is H2O. It is composed of two hydrogen atoms (H) and one oxygen atom (O).\n",
      "Processing query 4/10: How many continents are there on Earth?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "How many continents are there on Earth?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 4: There are 7 continents on Earth: Africa, Antarctica, Asia, Australia, Europe, North America, and South America. Would you like to know more about any of these continents?\n",
      "Processing query 5/10: What is the largest planet in our solar system?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the largest planet in our solar system?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 5: The largest planet in our solar system is Jupiter. It has a diameter of approximately 142,984 kilometers (88,846 miles). Jupiter is a gas giant, meaning it is primarily composed of hydrogen and helium gases. It is also known for its distinctive banded appearance, which is caused by strong winds in its atmosphere. Jupiter is the fifth planet from the Sun and is one of the most massive planets in the solar system, with a mass of approximately 318 times that of Earth.\n",
      "Processing query 6/10: Who painted the Mona Lisa?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who painted the Mona Lisa?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 6: The Mona Lisa was painted by the Italian artist Leonardo da Vinci. He created the painting in the early 16th century, and it is widely considered to be one of the most famous paintings in the world. It is a portrait of a woman who is thought to be Lisa Gherardini, the wife of a wealthy merchant named Francesco del Giocondo. The painting is known for its enigmatic smile and the intricate details of the subject's face and clothing. It is considered a masterpiece of\n",
      "Processing query 7/10: What is the square root of 64?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the square root of 64?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 7: The square root of 64 is 8, since 8 multiplied by 8 equals 64.\n",
      "Processing query 8/10: What is the currency of Japan?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the currency of Japan?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 8: The currency of Japan is the Japanese yen (JPY).\n",
      "Processing query 9/10: Who discovered penicillin?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who discovered penicillin?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 9: Penicillin was discovered by Scottish scientist Alexander Fleming in 1928. He accidentally left a petri dish of bacteria uncovered, allowing mold to grow and kill the surrounding bacteria. This led to the discovery of the antibacterial properties of penicillin, revolutionizing the treatment of bacterial infections.\n",
      "Processing query 10/10: What is the speed of light in a vacuum?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the speed of light in a vacuum?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 10: The speed of light in a vacuum is approximately 299,792,458 meters per second (m/s) or 186,282 miles per second (mi/s). This is a fundamental constant of the universe and is denoted by the letter c. It is the fastest speed at which any object or information can travel in a vacuum.\n",
      "Finished generating predictions.\n",
      "Evaluating predictions...\n",
      "Evaluating prediction 1/10\n",
      "Evaluation result for prediction 1: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is accurate and directly addresses the user\\'s question. The answer provided, \"Paris,\" is correct according to the ground truth. The greeting \"Bonjour!\" is a polite and contextually appropriate addition that does not detract from the accuracy of the response.\\nRating: [[10]]', 'score': 10}\n",
      "Evaluating prediction 2/10\n",
      "Evaluation result for prediction 2: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is accurate and correctly identifies William Shakespeare as the author of \"Romeo and Juliet\". It also provides additional context about the play, including its themes and cultural significance, which enhances the response without detracting from its accuracy. Therefore, the response deserves a high score.\\nRating: [[5]]', 'score': 5}\n",
      "Evaluating prediction 3/10\n",
      "Evaluation result for prediction 3: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and aligns perfectly with the reference. It correctly states that the chemical symbol for water is H₂O and provides additional relevant information about its composition.\\n\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 4/10\n",
      "Evaluation result for prediction 4: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly lists all seven continents and offers additional engagement by asking if the user would like more information.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 5/10\n",
      "ValueError during evaluation of prediction 5: Invalid output: \n",
      "\n",
      "[Evaluation]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 5 (Attempt 2/3)...\n",
      "ValueError during evaluation of prediction 5: Invalid output: \n",
      "\n",
      "[Evaluation]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 5 (Attempt 3/3)...\n",
      "Evaluation result for prediction 5: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It provides detailed information about Jupiter, including its size, composition, and characteristics. The response also includes additional relevant information about Jupiter's position in the solar system and its mass compared to Earth.\\n\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 6/10\n",
      "Evaluation result for prediction 6: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the ground truth. The response correctly identifies Leonardo da Vinci as the painter of the Mona Lisa and provides additional relevant information about the painting.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 7/10\n",
      "ValueError during evaluation of prediction 7: Invalid output: \n",
      "\n",
      "[Evaluation]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 7 (Attempt 2/3)...\n",
      "Evaluation result for prediction 7: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and directly addresses the user's question. The explanation is clear and correct, stating that the square root of 64 is 8 because 8 multiplied by 8 equals 64. This aligns perfectly with the ground truth.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 8/10\n",
      "Evaluation result for prediction 8: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is accurate and directly addresses the user\\'s question. The answer provided, \"Japanese yen (JPY),\" aligns perfectly with the ground truth, which is \"Yen.\" The response is clear, concise, and provides the correct information without any errors or omissions.\\n\\nRating: [[5]]', 'score': 5}\n",
      "Evaluating prediction 9/10\n",
      "Evaluation result for prediction 9: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly identifies Alexander Fleming as the discoverer of penicillin and provides additional context about the discovery. The information is factually correct and relevant to the question.\\n\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 10/10\n",
      "Evaluation result for prediction 10: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and aligns perfectly with the reference. It provides the correct speed of light in a vacuum in meters per second and also includes additional relevant information about the speed of light.\\n\\nRating: [[10]]\", 'score': 10}\n",
      "Finished evaluating predictions.\n",
      "Results updated for model: LLAMA_3_8B_INSTRUCT\n",
      "Evaluating with model: GRANITE_3_8B_INSTRUCT\n",
      "Initializing LLM with model ID: ibm/granite-3-8b-instruct\n",
      "LLM ibm/granite-3-8b-instruct initialized.\n",
      "Generating predictions...\n",
      "Processing query 1/10: What is the capital city of France?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital city of France?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 1: The capital city of France is Paris.\n",
      "Processing query 2/10: Who wrote the play \"Romeo and Juliet\"?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who wrote the play \"Romeo and Juliet\"?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 2: William Shakespeare wrote the play \"Romeo and Juliet\". It is one of his most famous works, a tragedy that tells the story of two young lovers from feuding families. The play is believed to have been written around 1595 or 1596.\n",
      "Processing query 3/10: What is the chemical symbol for water?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical symbol for water?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 3: The chemical symbol for water is H2O. This symbol represents two hydrogen atoms (H) bonded to one oxygen atom (O). Water is a fundamental compound that plays a crucial role in various chemical and biological processes.\n",
      "Processing query 4/10: How many continents are there on Earth?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "How many continents are there on Earth?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 4: There are seven continents on Earth: Africa, Antarctica, Asia, Australia, Europe, North America, and South America.\n",
      "Processing query 5/10: What is the largest planet in our solar system?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the largest planet in our solar system?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 5: The largest planet in our solar system is Jupiter. It is known for its Great Red Spot, a storm that has been raging on the planet for at least 300 years. Jupiter is also the fifth planet from the sun and is a gas giant, meaning it is composed mainly of gases and liquids.\n",
      "Processing query 6/10: Who painted the Mona Lisa?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who painted the Mona Lisa?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 6: The Mona Lisa was painted by the Italian artist Leonardo da Vinci. It is one of the most famous and iconic paintings in the world, known for its subject's enigmatic smile and the artist's masterful use of sfumato technique. The painting is housed at the Louvre Museum in Paris, France.\n",
      "Processing query 7/10: What is the square root of 64?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the square root of 64?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 7: The square root of 64 is 8. This is because 8 multiplied by itself equals 64.\n",
      "Processing query 8/10: What is the currency of Japan?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the currency of Japan?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 8: The currency of Japan is the Japanese Yen. It is the third most traded currency in the foreign exchange market after the United States dollar and the euro. The Japanese Yen is represented by the symbol ¥ and its ISO code is JPY. It is divided into 100 sen or 1000 rin. However, sen and rin are no longer used in everyday transactions.\n",
      "Processing query 9/10: Who discovered penicillin?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "Who discovered penicillin?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 9: The discovery of penicillin is often attributed to Sir Alexander Fleming, a Scottish scientist. In 1928, Fleming observed that a mold called Penicillium notatum had inhibited the growth of Staphylococcus bacteria in a petri dish. This observation led to the development of penicillin as the first antibiotic drug.\n",
      "\n",
      "However, it's important\n",
      "Processing query 10/10: What is the speed of light in a vacuum?\n",
      "Formatted Prompt:\n",
      "\n",
      "You are an expert assistant that provides helpful and accurate answers.\n",
      "\n",
      "### Instruction:\n",
      "What is the speed of light in a vacuum?\n",
      "\n",
      "### Response:\n",
      "\n",
      "Prediction for query 10: The speed of light in a vacuum is approximately 299,792 kilometers per second (km/s) or 186,282 miles per second (mi/s). This value is often denoted by the symbol \"c\" in physics equations. It is a fundamental constant of nature and is the maximum speed at which information or matter can travel according to the theory of relativity.\n",
      "Finished generating predictions.\n",
      "Evaluating predictions...\n",
      "Evaluating prediction 1/10\n",
      "Evaluation result for prediction 1: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is accurate and directly addresses the user\\'s question. The answer provided, \"Paris,\" is the correct capital city of France.\\n\\nRating: [[10]]', 'score': 10}\n",
      "Evaluating prediction 2/10\n",
      "ValueError during evaluation of prediction 2: Invalid output: \n",
      "\n",
      "[Evaluation]. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 2 (Attempt 2/3)...\n",
      "Evaluation result for prediction 2: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s answer is completely accurate as it correctly identifies William Shakespeare as the author of \"Romeo and Juliet\". It also provides additional relevant information about the play, such as its approximate creation date and a brief summary of the plot. This aligns perfectly with the ground truth.\\nRating: [[5]]', 'score': 5}\n",
      "Evaluating prediction 3/10\n",
      "Evaluation result for prediction 3: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and aligns perfectly with the reference. It correctly identifies the chemical symbol for water as H₂O and provides additional context about the composition of water molecules and their importance.\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 4/10\n",
      "Evaluation result for prediction 4: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly lists all seven continents.\\n\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 5/10\n",
      "Evaluation result for prediction 5: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's answer is accurate and aligns perfectly with the reference. It correctly identifies Jupiter as the largest planet in our solar system and provides additional relevant information about Jupiter's Great Red Spot, its position from the sun, and its composition as a gas giant.\\n\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 6/10\n",
      "Evaluation result for prediction 6: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly identifies Leonardo da Vinci as the painter of the Mona Lisa and provides additional relevant information about the painting.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 7/10\n",
      "Evaluation result for prediction 7: {'reasoning': \"\\n\\n[System Message]\\nPlease enter your evaluation by providing a short explanation first, and then rate the response on a scale of 1 to 10.\\n\\nAssistant: The assistant's answer is accurate and aligns perfectly with the reference. The explanation provided is also correct, as 8 multiplied by itself indeed equals 64.\\n\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 8/10\n",
      "Evaluation result for prediction 8: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and provides additional relevant information. It correctly identifies the currency of Japan as the Japanese Yen and provides details about its symbol, ISO code, and subdivisions. The additional information about its trading status and the historical subdivisions (sen and rin) enhances the response without introducing any inaccuracies.\\nRating: [[10]]\", 'score': 10}\n",
      "Evaluating prediction 9/10\n",
      "Evaluation result for prediction 9: {'reasoning': \"\\n\\n[Evaluation]\\nThe assistant's response accurately identifies Alexander Fleming as the discoverer of penicillin, which aligns perfectly with the reference. The additional details about the year of discovery, the type of mold, and the bacteria involved further enhance the accuracy and relevance of the response.\\n\\nRating: [[5]]\", 'score': 5}\n",
      "Evaluating prediction 10/10\n",
      "ValueError during evaluation of prediction 10: Invalid output: \n",
      "\n",
      "[Ground Truth]\n",
      "Approximately 299,792,458 meters per second\n",
      "\n",
      "[Evaluation]\n",
      "The assistant's response is mostly accurate but contains a minor error in the unit of measurement. The reference provides the speed of light in meters per second (m/s), while the assistant's answer uses kilometers per second (km/s). Additionally, the assistant includes an equivalent value in miles per second, which is not. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 10 (Attempt 2/3)...\n",
      "ValueError during evaluation of prediction 10: Invalid output: \n",
      "\n",
      "[Ground truth]\n",
      "Approximately 299,792,458 meters per second\n",
      "\n",
      "[Evaluation]\n",
      "The assistant's response is mostly accurate but contains a minor error in the units used. The ground truth provides the speed of light in meters per second, while the assistant's response uses kilometers per second. However, the numerical value and the context provided are correct. Therefore, the response aligns with the reference but has a. Output must contain a double bracketed string                 with the verdict between 1 and 10.\n",
      "Retrying evaluation for prediction 10 (Attempt 3/3)...\n",
      "Evaluation result for prediction 10: {'reasoning': '\\n\\n[Evaluation]\\nThe assistant\\'s response is mostly accurate but contains a minor error. The speed of light is given in kilometers per second (km/s), not meters per second (m/s) as stated in the ground truth. However, the numerical value provided is correct when converted to kilometers per second. The additional information about the symbol \"c\" and the theory of relativity is relevant and enhances the response.\\nRating: [[4]]', 'score': 4}\n",
      "Finished evaluating predictions.\n",
      "Results updated for model: GRANITE_3_8B_INSTRUCT\n",
      "Displaying results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    main()\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MISTRAL_LARGE\",\n          \"LLAMA_3_8B_INSTRUCT\",\n          \"GRANITE_3_8B_INSTRUCT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Who discovered penicillin?\",\n          \"Who wrote the play \\\"Romeo and Juliet\\\"?\",\n          \"Who painted the Mona Lisa?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"The discovery of penicillin is often attributed to Sir Alexander Fleming, a Scottish scientist. In 1928, Fleming observed that a mold called Penicillium notatum had inhibited the growth of Staphylococcus bacteria in a petri dish. This observation led to the development of penicillin as the first antibiotic drug.\\n\\nHowever, it's important\",\n          \"The square root of 64 is 8, since 8 multiplied by 8 equals 64.\",\n          \"The chemical symbol for water is H2O. It is composed of two hydrogen atoms (H) and one oxygen atom (O).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Alexander Fleming\",\n          \"William Shakespeare\",\n          \"Leonardo da Vinci\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Score\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 4,\n        \"max\": 10,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10,\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Feedback\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and provides additional relevant information. It correctly identifies the currency of Japan as the Japanese Yen and provides details about its symbol, ISO code, and subdivisions. The additional information about its trading status and the historical subdivisions (sen and rin) enhances the response without introducing any inaccuracies.\\nRating: [[10]]\",\n          \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the ground truth. The response correctly identifies Leonardo da Vinci as the painter of the Mona Lisa and provides additional relevant information about the painting.\\nRating: [[10]]\",\n          \"\\n\\n[Evaluation]\\nThe assistant's response is accurate and aligns perfectly with the reference. It correctly lists all seven continents.\\n\\nRating: [[10]]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-dd074ff4-4632-4d33-953f-b63ed89667e1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Query</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Score</th>\n",
       "      <th>Feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the capital city of France?</td>\n",
       "      <td>The capital city of France is Paris.</td>\n",
       "      <td>Paris</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>Who wrote the play \"Romeo and Juliet\"?</td>\n",
       "      <td>William Shakespeare wrote the play \"Romeo and ...</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the chemical symbol for water?</td>\n",
       "      <td>The chemical symbol for water is H₂O.</td>\n",
       "      <td>H₂O</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>How many continents are there on Earth?</td>\n",
       "      <td>There are 7 continents on Earth. They are:\\n1....</td>\n",
       "      <td>Seven</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>The largest planet in our solar system is Jupi...</td>\n",
       "      <td>Jupiter</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>Who painted the Mona Lisa?</td>\n",
       "      <td>The Mona Lisa was painted by the renowned Ital...</td>\n",
       "      <td>Leonardo da Vinci</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the square root of 64?</td>\n",
       "      <td>The square root of 64 is 8.</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the currency of Japan?</td>\n",
       "      <td>The currency of Japan is the Japanese Yen.</td>\n",
       "      <td>Yen</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>Who discovered penicillin?</td>\n",
       "      <td>Sir Alexander Fleming discovered penicillin. I...</td>\n",
       "      <td>Alexander Fleming</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MISTRAL_LARGE</td>\n",
       "      <td>What is the speed of light in a vacuum?</td>\n",
       "      <td>The speed of light in a vacuum is exactly 299,...</td>\n",
       "      <td>Approximately 299,792,458 meters per second</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[System Message]\\nPlease be as objective a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the capital city of France?</td>\n",
       "      <td>Bonjour! The capital city of France is Paris.</td>\n",
       "      <td>Paris</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>Who wrote the play \"Romeo and Juliet\"?</td>\n",
       "      <td>William Shakespeare wrote the play \"Romeo and ...</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the chemical symbol for water?</td>\n",
       "      <td>The chemical symbol for water is H2O. It is co...</td>\n",
       "      <td>H₂O</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>How many continents are there on Earth?</td>\n",
       "      <td>There are 7 continents on Earth: Africa, Antar...</td>\n",
       "      <td>Seven</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>The largest planet in our solar system is Jupi...</td>\n",
       "      <td>Jupiter</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>Who painted the Mona Lisa?</td>\n",
       "      <td>The Mona Lisa was painted by the Italian artis...</td>\n",
       "      <td>Leonardo da Vinci</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the square root of 64?</td>\n",
       "      <td>The square root of 64 is 8, since 8 multiplied...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the currency of Japan?</td>\n",
       "      <td>The currency of Japan is the Japanese yen (JPY).</td>\n",
       "      <td>Yen</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>Who discovered penicillin?</td>\n",
       "      <td>Penicillin was discovered by Scottish scientis...</td>\n",
       "      <td>Alexander Fleming</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LLAMA_3_8B_INSTRUCT</td>\n",
       "      <td>What is the speed of light in a vacuum?</td>\n",
       "      <td>The speed of light in a vacuum is approximatel...</td>\n",
       "      <td>Approximately 299,792,458 meters per second</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the capital city of France?</td>\n",
       "      <td>The capital city of France is Paris.</td>\n",
       "      <td>Paris</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>Who wrote the play \"Romeo and Juliet\"?</td>\n",
       "      <td>William Shakespeare wrote the play \"Romeo and ...</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the chemical symbol for water?</td>\n",
       "      <td>The chemical symbol for water is H2O. This sym...</td>\n",
       "      <td>H₂O</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>How many continents are there on Earth?</td>\n",
       "      <td>There are seven continents on Earth: Africa, A...</td>\n",
       "      <td>Seven</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>The largest planet in our solar system is Jupi...</td>\n",
       "      <td>Jupiter</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's answer is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>Who painted the Mona Lisa?</td>\n",
       "      <td>The Mona Lisa was painted by the Italian artis...</td>\n",
       "      <td>Leonardo da Vinci</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the square root of 64?</td>\n",
       "      <td>The square root of 64 is 8. This is because 8 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[System Message]\\nPlease enter your evalua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the currency of Japan?</td>\n",
       "      <td>The currency of Japan is the Japanese Yen. It ...</td>\n",
       "      <td>Yen</td>\n",
       "      <td>10</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>Who discovered penicillin?</td>\n",
       "      <td>The discovery of penicillin is often attribute...</td>\n",
       "      <td>Alexander Fleming</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GRANITE_3_8B_INSTRUCT</td>\n",
       "      <td>What is the speed of light in a vacuum?</td>\n",
       "      <td>The speed of light in a vacuum is approximatel...</td>\n",
       "      <td>Approximately 299,792,458 meters per second</td>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\n[Evaluation]\\nThe assistant's response is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd074ff4-4632-4d33-953f-b63ed89667e1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-dd074ff4-4632-4d33-953f-b63ed89667e1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-dd074ff4-4632-4d33-953f-b63ed89667e1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1a010753-3752-46ce-87f1-897c40cf484d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a010753-3752-46ce-87f1-897c40cf484d')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1a010753-3752-46ce-87f1-897c40cf484d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                    Model                                            Query  \\\n",
       "0           MISTRAL_LARGE              What is the capital city of France?   \n",
       "1           MISTRAL_LARGE           Who wrote the play \"Romeo and Juliet\"?   \n",
       "2           MISTRAL_LARGE           What is the chemical symbol for water?   \n",
       "3           MISTRAL_LARGE          How many continents are there on Earth?   \n",
       "4           MISTRAL_LARGE  What is the largest planet in our solar system?   \n",
       "5           MISTRAL_LARGE                       Who painted the Mona Lisa?   \n",
       "6           MISTRAL_LARGE                   What is the square root of 64?   \n",
       "7           MISTRAL_LARGE                   What is the currency of Japan?   \n",
       "8           MISTRAL_LARGE                       Who discovered penicillin?   \n",
       "9           MISTRAL_LARGE          What is the speed of light in a vacuum?   \n",
       "10    LLAMA_3_8B_INSTRUCT              What is the capital city of France?   \n",
       "11    LLAMA_3_8B_INSTRUCT           Who wrote the play \"Romeo and Juliet\"?   \n",
       "12    LLAMA_3_8B_INSTRUCT           What is the chemical symbol for water?   \n",
       "13    LLAMA_3_8B_INSTRUCT          How many continents are there on Earth?   \n",
       "14    LLAMA_3_8B_INSTRUCT  What is the largest planet in our solar system?   \n",
       "15    LLAMA_3_8B_INSTRUCT                       Who painted the Mona Lisa?   \n",
       "16    LLAMA_3_8B_INSTRUCT                   What is the square root of 64?   \n",
       "17    LLAMA_3_8B_INSTRUCT                   What is the currency of Japan?   \n",
       "18    LLAMA_3_8B_INSTRUCT                       Who discovered penicillin?   \n",
       "19    LLAMA_3_8B_INSTRUCT          What is the speed of light in a vacuum?   \n",
       "20  GRANITE_3_8B_INSTRUCT              What is the capital city of France?   \n",
       "21  GRANITE_3_8B_INSTRUCT           Who wrote the play \"Romeo and Juliet\"?   \n",
       "22  GRANITE_3_8B_INSTRUCT           What is the chemical symbol for water?   \n",
       "23  GRANITE_3_8B_INSTRUCT          How many continents are there on Earth?   \n",
       "24  GRANITE_3_8B_INSTRUCT  What is the largest planet in our solar system?   \n",
       "25  GRANITE_3_8B_INSTRUCT                       Who painted the Mona Lisa?   \n",
       "26  GRANITE_3_8B_INSTRUCT                   What is the square root of 64?   \n",
       "27  GRANITE_3_8B_INSTRUCT                   What is the currency of Japan?   \n",
       "28  GRANITE_3_8B_INSTRUCT                       Who discovered penicillin?   \n",
       "29  GRANITE_3_8B_INSTRUCT          What is the speed of light in a vacuum?   \n",
       "\n",
       "                                           Prediction  \\\n",
       "0                The capital city of France is Paris.   \n",
       "1   William Shakespeare wrote the play \"Romeo and ...   \n",
       "2               The chemical symbol for water is H₂O.   \n",
       "3   There are 7 continents on Earth. They are:\\n1....   \n",
       "4   The largest planet in our solar system is Jupi...   \n",
       "5   The Mona Lisa was painted by the renowned Ital...   \n",
       "6                         The square root of 64 is 8.   \n",
       "7          The currency of Japan is the Japanese Yen.   \n",
       "8   Sir Alexander Fleming discovered penicillin. I...   \n",
       "9   The speed of light in a vacuum is exactly 299,...   \n",
       "10      Bonjour! The capital city of France is Paris.   \n",
       "11  William Shakespeare wrote the play \"Romeo and ...   \n",
       "12  The chemical symbol for water is H2O. It is co...   \n",
       "13  There are 7 continents on Earth: Africa, Antar...   \n",
       "14  The largest planet in our solar system is Jupi...   \n",
       "15  The Mona Lisa was painted by the Italian artis...   \n",
       "16  The square root of 64 is 8, since 8 multiplied...   \n",
       "17   The currency of Japan is the Japanese yen (JPY).   \n",
       "18  Penicillin was discovered by Scottish scientis...   \n",
       "19  The speed of light in a vacuum is approximatel...   \n",
       "20               The capital city of France is Paris.   \n",
       "21  William Shakespeare wrote the play \"Romeo and ...   \n",
       "22  The chemical symbol for water is H2O. This sym...   \n",
       "23  There are seven continents on Earth: Africa, A...   \n",
       "24  The largest planet in our solar system is Jupi...   \n",
       "25  The Mona Lisa was painted by the Italian artis...   \n",
       "26  The square root of 64 is 8. This is because 8 ...   \n",
       "27  The currency of Japan is the Japanese Yen. It ...   \n",
       "28  The discovery of penicillin is often attribute...   \n",
       "29  The speed of light in a vacuum is approximatel...   \n",
       "\n",
       "                                      Reference Score  \\\n",
       "0                                         Paris    10   \n",
       "1                           William Shakespeare    10   \n",
       "2                                           H₂O    10   \n",
       "3                                         Seven    10   \n",
       "4                                       Jupiter    10   \n",
       "5                             Leonardo da Vinci     5   \n",
       "6                                             8    10   \n",
       "7                                           Yen     5   \n",
       "8                             Alexander Fleming    10   \n",
       "9   Approximately 299,792,458 meters per second    10   \n",
       "10                                        Paris    10   \n",
       "11                          William Shakespeare     5   \n",
       "12                                          H₂O     5   \n",
       "13                                        Seven    10   \n",
       "14                                      Jupiter    10   \n",
       "15                            Leonardo da Vinci    10   \n",
       "16                                            8    10   \n",
       "17                                          Yen     5   \n",
       "18                            Alexander Fleming     5   \n",
       "19  Approximately 299,792,458 meters per second    10   \n",
       "20                                        Paris    10   \n",
       "21                          William Shakespeare     5   \n",
       "22                                          H₂O     5   \n",
       "23                                        Seven    10   \n",
       "24                                      Jupiter     5   \n",
       "25                            Leonardo da Vinci    10   \n",
       "26                                            8    10   \n",
       "27                                          Yen    10   \n",
       "28                            Alexander Fleming     5   \n",
       "29  Approximately 299,792,458 meters per second     4   \n",
       "\n",
       "                                             Feedback  \n",
       "0   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "1   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "2   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "3   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "4   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "5   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "6   \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "7   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "8   \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "9   \\n\\n[System Message]\\nPlease be as objective a...  \n",
       "10  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "11  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "12  \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "13  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "14  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "15  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "16  \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "17  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "18  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "19  \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "20  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "21  \\n\\n[Evaluation]\\nThe assistant's answer is co...  \n",
       "22  \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "23  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "24  \\n\\n[Evaluation]\\nThe assistant's answer is ac...  \n",
       "25  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "26  \\n\\n[System Message]\\nPlease enter your evalua...  \n",
       "27  \\n\\n[Evaluation]\\nThe assistant's response is ...  \n",
       "28  \\n\\n[Evaluation]\\nThe assistant's response acc...  \n",
       "29  \\n\\n[Evaluation]\\nThe assistant's response is ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to 'evaluation_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum, auto\n",
    "from langchain import PromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from IPython.display import display\n",
    "from typing import List, Dict\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Configuration Parameters for Generation LLMs\n",
    "GENERATION_PARAMETERS = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"###\"],\n",
    "}\n",
    "\n",
    "# Configuration Parameters for Evaluator LLM\n",
    "EVALUATOR_PARAMETERS = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"###\"],\n",
    "}\n",
    "\n",
    "# Evaluation Criteria for Multiple Metrics\n",
    "CRITERIA = {\n",
    "    \"Accuracy\": \"\"\"\n",
    "Score 1: The translation does not convey the original meaning at all.\n",
    "Score 2: The translation partially conveys the original meaning but misses key elements.\n",
    "Score 3: The translation conveys the general meaning with some inaccuracies.\n",
    "Score 4: The translation accurately conveys the original meaning with minor errors.\n",
    "Score 5: The translation perfectly conveys the original meaning without any errors.\"\"\",\n",
    "    \"Fluency\": \"\"\"\n",
    "Score 1: The translation is incomprehensible with numerous grammatical errors.\n",
    "Score 2: The translation has frequent grammatical errors that impede understanding.\n",
    "Score 3: The translation has some grammatical errors but is generally understandable.\n",
    "Score 4: The translation is mostly fluent with minor grammatical errors.\n",
    "Score 5: The translation is completely fluent with no grammatical errors.\"\"\",\n",
    "    \"Terminology\": \"\"\"\n",
    "Score 1: The translation uses incorrect or inconsistent terminology throughout.\n",
    "Score 2: The translation has some incorrect or inconsistent terminology.\n",
    "Score 3: The translation uses correct terminology with occasional inconsistencies.\n",
    "Score 4: The translation consistently uses correct terminology with minor lapses.\n",
    "Score 5: The translation consistently and accurately uses the correct terminology.\"\"\",\n",
    "    \"Style\": \"\"\"\n",
    "Score 1: The translation style is inappropriate and significantly differs from the original.\n",
    "Score 2: The translation style somewhat differs from the original, affecting readability.\n",
    "Score 3: The translation maintains the original style with some deviations.\n",
    "Score 4: The translation closely matches the original style with minor differences.\n",
    "Score 5: The translation perfectly matches the original style, ensuring ease of reading and understanding.\"\"\",\n",
    "    \"Formatting\": \"\"\"\n",
    "Score 1: The translation has major formatting issues, making it hard to read.\n",
    "Score 2: The translation has several formatting inconsistencies.\n",
    "Score 3: The translation has minor formatting issues but is generally well-formatted.\n",
    "Score 4: The translation is well-formatted with slight inconsistencies.\n",
    "Score 5: The translation is perfectly formatted, matching the original design.\"\"\",\n",
    "}\n",
    "\n",
    "# Dataset File Path\n",
    "DATASET_FILE_PATH = \"input-data.xlsx\"  # Replace with your file path\n",
    "\n",
    "# Constants for Retry Mechanism\n",
    "MAX_RETRIES = 3  # Maximum number of retry attempts\n",
    "\n",
    "# Define the PromptTemplate with system and user prompts\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert assistant that provides helpful and accurate answers.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\",\n",
    "    input_variables=[\"instruction\"],\n",
    ")\n",
    "\n",
    "# Example Selection of Models\n",
    "# Ensure that WatsonXModel enum is defined appropriately elsewhere in your code\n",
    "selected_models = [\n",
    "    WatsonXModel.MISTRAL_LARGE,\n",
    "    WatsonXModel.LLAMA_3_8B_INSTRUCT,\n",
    "    WatsonXModel.GRANITE_3_8B_INSTRUCT,\n",
    "]\n",
    "\n",
    "\n",
    "def get_evaluation_criteria(custom_criteria: Dict[str, str] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Combine default criteria with any custom criteria provided.\n",
    "    \"\"\"\n",
    "    default_criteria = CRITERIA.copy()\n",
    "    if custom_criteria:\n",
    "        default_criteria.update(custom_criteria)\n",
    "    print(\"Evaluation criteria loaded.\")\n",
    "    return default_criteria\n",
    "\n",
    "\n",
    "def load_queries_from_excel(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load queries and expected answers from an Excel file.\n",
    "    \"\"\"\n",
    "    print(f\"Loading queries from Excel file: {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "        raise\n",
    "\n",
    "    required_columns = [\"Query\", \"Expected\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    print(f\"Loaded {len(df)} queries from Excel.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def initialize_llm(model_id: str, parameters: Dict) -> WatsonxLLM:\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM with the given model ID and parameters.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing LLM with model ID: {model_id}\")\n",
    "    try:\n",
    "        llm = WatsonxLLM(\n",
    "            model_id=model_id,\n",
    "            url=WATSONX_URL,  # Ensure WATSONX_URL is defined\n",
    "            project_id=WATSONX_PROJECT_ID,  # Ensure WATSONX_PROJECT_ID is defined\n",
    "            params=parameters,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize LLM {model_id}: {e}\")\n",
    "        raise\n",
    "    print(f\"LLM {model_id} initialized.\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def generate_predictions(llm: WatsonxLLM, queries: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate predictions for a list of queries using the provided LLM.\n",
    "    Utilizes the prompt template and includes a stop sequence.\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = []\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"Processing query {i}/{len(queries)}: {query}\")\n",
    "        try:\n",
    "            # Format the prompt using the prompt template\n",
    "            formatted_prompt = prompt_template.format(instruction=query)\n",
    "            print(f\"Formatted Prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "            # Invoke the LLM with the formatted prompt and stop sequence\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            # Optionally, remove the stop sequence from the response\n",
    "            for stop_seq in llm.params.get(\"stop_sequences\", []):\n",
    "                response = response.split(stop_seq)[0].strip()\n",
    "\n",
    "            print(f\"Prediction for query {i}: {response}\")\n",
    "            predictions.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction for query {i}: {e}\")\n",
    "            predictions.append(\"Error generating prediction\")\n",
    "    print(\"Finished generating predictions.\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    evaluator,\n",
    "    predictions: List[str],\n",
    "    references: List[str],\n",
    "    inputs: List[str],\n",
    "    criteria: Dict[str, str],\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against references using the provided evaluator for each metric.\n",
    "    Implements a retry mechanism for robustness.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating predictions across multiple metrics...\")\n",
    "    eval_results = []\n",
    "    for i, (pred, ref, inp) in enumerate(zip(predictions, references, inputs), 1):\n",
    "        print(f\"Evaluating prediction {i}/{len(predictions)}\")\n",
    "        for metric, description in criteria.items():\n",
    "            print(f\"  Evaluating Metric: {metric}\")\n",
    "            attempt = 0\n",
    "            success = False\n",
    "            while attempt < MAX_RETRIES and not success:\n",
    "                try:\n",
    "                    # Construct instruction for the evaluator specific to the metric\n",
    "                    evaluation_instruction = f\"\"\"\n",
    "Evaluate the following translation based on the metric: {metric}\n",
    "\n",
    "### Metric Description:\n",
    "{description}\n",
    "\n",
    "### Original Text:\n",
    "{inp}\n",
    "\n",
    "### Translated Text:\n",
    "{pred}\n",
    "\n",
    "### Reference Translation:\n",
    "{ref}\n",
    "\n",
    "### Provide a score between 1 and 5 and a brief explanation.\n",
    "\"\"\"\n",
    "                    # Format the prompt for the evaluator\n",
    "                    formatted_prompt = prompt_template.format(\n",
    "                        instruction=evaluation_instruction\n",
    "                    )\n",
    "                    print(f\"  Formatted Evaluation Prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "                    # Invoke the evaluator LLM\n",
    "                    response = evaluator.invoke(formatted_prompt)\n",
    "                    # Remove stop sequences if any\n",
    "                    for stop_seq in evaluator.params.get(\"stop_sequences\", []):\n",
    "                        response = response.split(stop_seq)[0].strip()\n",
    "\n",
    "                    # Parse the response to extract score and feedback\n",
    "                    # Assuming the response starts with \"Score X: explanation\"\n",
    "                    if response.lower().startswith(\"score\"):\n",
    "                        parts = response.split(\":\", 1)\n",
    "                        if len(parts) == 2:\n",
    "                            score_part, feedback = parts\n",
    "                            score = score_part.split()[1].strip()\n",
    "                            feedback = feedback.strip()\n",
    "                        else:\n",
    "                            score = \"N/A\"\n",
    "                            feedback = \"Invalid response format.\"\n",
    "                    else:\n",
    "                        score = \"N/A\"\n",
    "                        feedback = \"Invalid response format.\"\n",
    "\n",
    "                    eval_results.append(\n",
    "                        {\"Metric\": metric, \"Score\": score, \"Feedback\": feedback}\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"    Evaluation Result for Metric '{metric}': Score={score}, Feedback={feedback}\"\n",
    "                    )\n",
    "                    success = True\n",
    "                except ValueError as ve:\n",
    "                    attempt += 1\n",
    "                    print(\n",
    "                        f\"    ValueError during evaluation of metric '{metric}': {ve}\"\n",
    "                    )\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        print(\n",
    "                            f\"    Retrying evaluation for metric '{metric}' (Attempt {attempt + 1}/{MAX_RETRIES})...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"    Max retries reached for metric '{metric}'. Assigning default evaluation results.\"\n",
    "                        )\n",
    "                        eval_results.append(\n",
    "                            {\n",
    "                                \"Metric\": metric,\n",
    "                                \"Score\": \"N/A\",\n",
    "                                \"Feedback\": \"Evaluation failed after multiple attempts.\",\n",
    "                            }\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    attempt += 1\n",
    "                    print(\n",
    "                        f\"    Unexpected error during evaluation of metric '{metric}': {e}\"\n",
    "                    )\n",
    "                    if attempt < MAX_RETRIES:\n",
    "                        print(\n",
    "                            f\"    Retrying evaluation for metric '{metric}' (Attempt {attempt + 1}/{MAX_RETRIES})...\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"    Max retries reached for metric '{metric}'. Assigning default evaluation results.\"\n",
    "                        )\n",
    "                        eval_results.append(\n",
    "                            {\n",
    "                                \"Metric\": metric,\n",
    "                                \"Score\": \"N/A\",\n",
    "                                \"Feedback\": \"Evaluation failed after multiple attempts due to an unexpected error.\",\n",
    "                            }\n",
    "                        )\n",
    "    print(\"Finished evaluating predictions.\")\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting evaluation process...\")\n",
    "\n",
    "    # Configuration Section\n",
    "    custom_criteria = {}  # Add custom criteria here if needed\n",
    "    criteria = get_evaluation_criteria(custom_criteria)\n",
    "\n",
    "    print(f\"Selected models: {[model.name for model in selected_models]}\")\n",
    "\n",
    "    # Load Queries from Excel\n",
    "    df = load_queries_from_excel(DATASET_FILE_PATH)\n",
    "    queries = df[\"Query\"].tolist()\n",
    "    print(f\"Loaded queries: {queries}\")\n",
    "    df[\"ExpectedList\"] = df[\"Expected\"].apply(\n",
    "        lambda x: [e.strip() for e in str(x).split(\";\")]\n",
    "    )\n",
    "    references = df[\"ExpectedList\"].apply(lambda x: x[0] if len(x) > 0 else \"\").tolist()\n",
    "    print(f\"Loaded references: {references}\")\n",
    "\n",
    "    # Initialize Evaluator LLM with EVALUATOR_PARAMETERS\n",
    "    try:\n",
    "        evaluator_llm = initialize_llm(\n",
    "            WatsonXModel.MISTRAL_LARGE.value,  # Assuming MISTRAL_LARGE is suitable for evaluation\n",
    "            EVALUATOR_PARAMETERS,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize evaluator LLM: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        evaluator = load_evaluator(\n",
    "            \"labeled_score_string\",\n",
    "            criteria=criteria,  # Although criteria are now handled manually, keeping this for compatibility\n",
    "            llm=evaluator_llm,  # Placeholder; actual LLM will be used per metric\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load evaluator: {e}\")\n",
    "        return\n",
    "    print(\"Evaluator initialized.\")\n",
    "\n",
    "    # Prepare a DataFrame to store results\n",
    "    results = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"Query\",\n",
    "            \"Prediction\",\n",
    "            \"Reference\",\n",
    "            \"Metric\",\n",
    "            \"Score\",\n",
    "            \"Feedback\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Iterate over each selected model\n",
    "    for model in selected_models:\n",
    "        print(f\"Evaluating with model: {model.name}\")\n",
    "        try:\n",
    "            llm = initialize_llm(model.value, GENERATION_PARAMETERS)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {model.name} due to initialization failure: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate predictions using the prompt template\n",
    "        predictions = generate_predictions(llm, queries)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        eval_results = evaluate_predictions(\n",
    "            evaluator, predictions, references, queries, criteria\n",
    "        )\n",
    "\n",
    "        # Since evaluate_predictions now returns a flat list with all metrics, we need to organize them per query\n",
    "        # Assuming that evaluate_predictions appends metrics in order for each query\n",
    "        # Calculate the number of metrics per query\n",
    "        num_metrics = len(criteria)\n",
    "        if len(eval_results) != len(queries) * num_metrics:\n",
    "            print(\"Warning: Mismatch in evaluation results length.\")\n",
    "\n",
    "        # Organize evaluation results per query\n",
    "        for i, query in enumerate(queries):\n",
    "            prediction = predictions[i]\n",
    "            reference = references[i]\n",
    "            for m in range(num_metrics):\n",
    "                result_index = i * num_metrics + m\n",
    "                if result_index < len(eval_results):\n",
    "                    eval_res = eval_results[result_index]\n",
    "                    results = results.append(\n",
    "                        {\n",
    "                            \"Model\": model.name,\n",
    "                            \"Query\": query,\n",
    "                            \"Prediction\": prediction,\n",
    "                            \"Reference\": reference,\n",
    "                            \"Metric\": eval_res[\"Metric\"],\n",
    "                            \"Score\": eval_res[\"Score\"],\n",
    "                            \"Feedback\": eval_res[\"Feedback\"],\n",
    "                        },\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                else:\n",
    "                    # In case of missing evaluation results\n",
    "                    results = results.append(\n",
    "                        {\n",
    "                            \"Model\": model.name,\n",
    "                            \"Query\": query,\n",
    "                            \"Prediction\": prediction,\n",
    "                            \"Reference\": reference,\n",
    "                            \"Metric\": \"N/A\",\n",
    "                            \"Score\": \"N/A\",\n",
    "                            \"Feedback\": \"Missing evaluation result.\",\n",
    "                        },\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "        print(f\"Results updated for model: {model.name}\")\n",
    "\n",
    "    # Display Results\n",
    "    print(\"Displaying results...\")\n",
    "    display(results)\n",
    "\n",
    "    # Optionally, save results to Excel\n",
    "    try:\n",
    "        results.to_excel(\"evaluation_results.xlsx\", index=False)\n",
    "        print(\"Evaluation results saved to 'evaluation_results.xlsx'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save evaluation results to Excel: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvkATiw4AbSB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation Criteria\n",
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 2: The answer has minor relevance but does not align with the reference.\n",
    "Score 3: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 4: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 5: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")\n",
    "\n",
    "# Accuracy: Is the meaning of the original text communicated clearly?\n",
    "# Fluency: Are standard spelling, punctuation and grammar used in the translation?\n",
    "# Terminology: Are keywords and phrases translated accurately and consistently?\n",
    "# Style: Is the translation easy to read and understand, and does it match the style of the original?\n",
    "# Design/formatting: Is everything formatted correctly in the translation?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
